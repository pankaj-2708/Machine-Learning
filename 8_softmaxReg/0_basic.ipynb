{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ca770a-2679-4d1e-b3e8-7076c675e979",
   "metadata": {},
   "source": [
    "Softmax regression, also known as multinomial logistic regression, is an extension of logistic regression used when the output has more than two classes. It functions similarly to logistic regression but is designed to handle multi-class classification problems.\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=1024&disable=upscale&auto=webp\"/>\n",
    "\n",
    "For example, suppose we have 3 output classes and 3 input features. In this case, the model will calculate 3 + 1 coefficients (3 weights and 1 bias) for each class, resulting in a total of 4 parameters per class. These are used to compute the raw outputs (logits), one for each class. These logits are then passed through the softmax function, as shown in the diagram above under the \"output layer,\" to convert them into probabilities. The model then predicts the class corresponding to the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acab9c-5237-4a0c-8a19-b5f2c1428b9e",
   "metadata": {},
   "source": [
    "One way to handle multi-class classification using softmax regression is by applying one-hot encoding to the output (target) column. In this approach, each class label is converted into a binary vector where only the index corresponding to the class is marked as 1 and the rest are 0. For example, if there are three classes — Class 0, Class 1, and Class 2 — the labels would be transformed as follows: 0 → [1, 0, 0], 1 → [0, 1, 0], and 2 → [0, 0, 1]. This encoding allows the model to treat the classification task as a probability distribution problem. The softmax function is then used in the final layer of the model to output a probability vector for each instance, where each value represents the predicted probability of the input belonging to a particular class. The class with the highest probability is selected as the model’s prediction. This process makes it possible for the model to learn distinct weights for each class and output interpretable probabilistic predictions in multi-class settings.\n",
    "\n",
    "but in this method we have to create a large no of features if large dataset \n",
    "\n",
    "so we modify our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ffb4d-c18a-441a-974c-5296ce061f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
